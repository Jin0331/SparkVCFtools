{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appname, folder name : \n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark function\n",
    "from pyspark import Row, StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import pandas_udf, udf, explode, array, when\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, BooleanType, StructType, StructField\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Python function\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from functools import reduce \n",
    "import copy\n",
    "\n",
    "appname = input(\"appname, folder name : \")\n",
    "#folder_name = copy.deepcopy(appname) \n",
    "#gvcf_count = int(input(\"gvcf count : \"))\n",
    "\n",
    "# Start for Spark Session\n",
    "spark = SparkSession.builder.master(\"spark://master:7077\")\\\n",
    "                        .appName(appname)\\\n",
    "                        .config(\"spark.driver.memory\", \"8G\")\\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"8G\")\\\n",
    "                        .config(\"spark.executor.memory\", \"24G\")\\\n",
    "                        .config(\"spark.executor.core\", 3)\\\n",
    "                        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "                        .config(\"spark.network.timeout\", \"9999s\")\\\n",
    "                        .config(\"spark.files.fetchTimeout\", \"9999s\")\\\n",
    "                        .config(\"spark.sql.shuffle.partitions\", 80)\\\n",
    "                        .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "                        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000000)\\\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadoop_list(length, hdfs):\n",
    "    args = \"hdfs dfs -ls \"+ hdfs +\" | awk '{print $8}'\"\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    all_dart_dirs = s_output.split()\n",
    "    \n",
    "    return all_dart_dirs[:length]\n",
    "\n",
    "def preVCF(hdfs, flag, spark):\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    #header_contig = vcf.filter(lambda x : re.match(\"^#\", x))\n",
    "    col_name = vcf.filter(lambda x : x.startswith(\"#CHROM\")).first().split(\"\\t\")\n",
    "    vcf_data = vcf.filter(lambda x : re.match(\"[^#][^#]\", x))\\\n",
    "                       .map(lambda x : x.split(\"\\t\"))\\\n",
    "                       .toDF(col_name)\\\n",
    "                       .withColumn(\"POS\", F.col(\"POS\").cast(IntegerType()))\\\n",
    "                       .drop(\"QUAL\",\"FILTER\")\n",
    "    if flag == 0 :\n",
    "        vcf_data = vcf_data.drop(vcf_data.columns[-1])\n",
    "    elif flag == 1:\n",
    "        vcf_data = vcf_data.selecet([\"#CHROM\", \"POS\"] + [vcf_data.columns[-1]])\n",
    "    return vcf_data\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for index in range(0, len(lst), n):\n",
    "        yield lst[index:index + n]\n",
    "        \n",
    "def addIndex(POS, size):\n",
    "    if POS == 1:\n",
    "        return POS\n",
    "    else :\n",
    "        return int(POS / size + 1) \n",
    "addIndex_udf = udf(addIndex, returnType=IntegerType())\n",
    "\n",
    "# for indel\n",
    "word_len = udf(lambda col : True if len(col) >= 2 else False, returnType=BooleanType())\n",
    "ref_melt = udf(lambda ref : list(ref)[1:], ArrayType(StringType()))    \n",
    "\n",
    "def ref_concat(temp): \n",
    "    return_str = []\n",
    "    for num in range(0, len(temp)):\n",
    "        return_str.append(temp[num] + \"_\" + str(int(num + 1)))\n",
    "    return return_str\n",
    "ref_concat = udf(ref_concat, ArrayType(StringType()))\n",
    "\n",
    "def info_change(temp):\n",
    "    some_list = temp.split(\";\")\n",
    "    result = [i for i in some_list if i.startswith('DP=')]\n",
    "    return result[0]\n",
    "info_change = udf(info_change, StringType())\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionByName, dfs) \n",
    "\n",
    "# for sample value\n",
    "value_change = udf(lambda value : \"./.\" + value[3:], StringType())\n",
    "\n",
    "# for POS index\n",
    "def sampling_func(data, ran):\n",
    "    N = len(data)\n",
    "    sample = data.take(range(0, N, ran))\n",
    "    return sample\n",
    "\n",
    "def sample_join(left, right):\n",
    "    return left.select(F.col(\"#CHROM\"), F.col(\"POS\"), left.columns[-1])\\\n",
    "        .join(right.select(F.col(\"#CHROM\"), F.col(\"POS\"), right.columns[-1]), [\"#CHROM\", \"POS\"], \"full\").orderBy(F.col(\"#CHROM\"), F.col(\"POS\"))\n",
    "\n",
    "def union_all(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "def outer_union_all(dfs):\n",
    "    all_cols = set([])\n",
    "    for df in dfs:\n",
    "        all_cols |= set(df.columns) \n",
    "    all_cols = list(all_cols)\n",
    "    print(all_cols)\n",
    "\n",
    "    def expr(cols, all_cols):\n",
    "\n",
    "        def append_cols(col):\n",
    "            if col in cols:\n",
    "                return col\n",
    "            else:\n",
    "                return F.lit(None).alias(col)\n",
    "\n",
    "        cols_ = map(append_cols, all_cols)\n",
    "        return list(cols_)\n",
    "\n",
    "    union_df = union_all([df.select(expr(df.columns, all_cols)) for df in dfs])\n",
    "    return union_df\n",
    "\n",
    "# pandas udf\n",
    "value_max = udf(lambda value : max(value), StringType())\n",
    "\n",
    "def info_min(info):\n",
    "    temp = [value for value in info if value.startswith(\"END=\") == False]\n",
    "    # info 값 수정\n",
    "    return \"%\".join(temp)\n",
    "info_min = udf(info_min, StringType())\n",
    "\n",
    "def format_value(value): \n",
    "    # ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=\"Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.\">\n",
    "    if len(value) == 1:\n",
    "        value.append(\"GT:DP:GQ:MIN_DP:PL\")\n",
    "    def format_reduce(left, right):\n",
    "        left, right = left.split(\":\"), right.split(\":\")\n",
    "        if len(left) <= len(right):        \n",
    "            temp = copy.deepcopy(right)\n",
    "            right = copy.deepcopy(left)\n",
    "            left = copy.deepcopy(temp)\n",
    "        for value in right:\n",
    "            if value not in left:\n",
    "                left.append(value)\n",
    "        return \":\".join(left)\n",
    "    return str(reduce(format_reduce, value))\n",
    "format_value = udf(format_value, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = \"hdfs://master:9000\"\n",
    "hdfs_list = hadoop_list(15, \"/raw_data/gvcf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286988405"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcf[preVCF(hdfs + sample.decode(\"UTF-8\"), 0, spark) for sample in hdfs_list]\n",
    "vcf = union_all([preVCF(hdfs + sample.decode(\"UTF-8\"), 0, spark) for sample in hdfs_list])\\\n",
    "            .groupBy(\"#CHROM\", \"POS\").agg(value_max(F.collect_set(\"ID\")).alias(\"ID\"), value_max(F.collect_set(\"REF\")).alias(\"REF\"), \n",
    "                                      value_max(F.collect_set(\"ALT\")).alias(\"ALT\"), info_min(F.collect_set(\"INFO\")).alias(\"INFO\"),\n",
    "                                      format_value(F.collect_set(\"FORMAT\")).alias(\"FORMAT\")).cache()\n",
    "vcf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = F.split(\"REF_temp\", '_')\n",
    "info_indel = vcf.filter(word_len(F.col(\"REF\")))\\\n",
    "            .withColumn(\"REF\", ref_melt(F.col(\"REF\"))).withColumn(\"REF\", ref_concat(F.col(\"REF\")))\\\n",
    "            .withColumn(\"REF\", explode(F.col(\"REF\"))).withColumnRenamed(\"REF\", \"REF_temp\")\\\n",
    "            .withColumn('REF', split_col.getItem(0)).withColumn('POS_var', split_col.getItem(1))\\\n",
    "            .drop(F.col(\"REF_temp\")).withColumn(\"POS\", (F.col(\"POS\") + F.col(\"POS_var\")).cast(IntegerType()))\\\n",
    "            .drop(F.col(\"POS_var\"))\\\n",
    "            .withColumn('ID', F.lit(\".\"))\\\n",
    "            .withColumn('ALT', F.lit(\"*,<NON_REF>\"))\\\n",
    "            .withColumn(\"INFO\", info_change(F.col(\"INFO\")))\\\n",
    "            .orderBy(F.col(\"#CHROM\"), F.col(\"POS\"))\\\n",
    "            .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf.filter(word_len(F.col(\"REF\"))).withColumn(\"REF\", ref_melt(F.col(\"REF\"))).withColumn(\"REF\", ref_concat(F.col(\"REF\")))\\\n",
    "   .withColumn(\"REF\", explode(F.col(\"REF\"))).withColumnRenamed(\"REF\", \"REF_temp\")\\\n",
    "   .withColumn('REF', split_col.getItem(0)).withColumn('POS_var', split_col.getItem(1))\\\n",
    "   .withColumn(\"POS\", (F.col(\"POS\") + F.col(\"POS_var\")).cast(IntegerType())).drop(\"REF_temp\",\"POS_var\")\\\n",
    "   .withColumn('ID', F.lit(\".\"))\\\n",
    "   .withColumn('ALT', F.lit(\"*,<NON_REF>\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_indel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<string>\", PandasUDFType.GROUPED_AGG)\n",
    "def value_collect(value):\n",
    "    return value\n",
    "\n",
    "@pandas_udf(\"String\", PandasUDFType.GROUPED_AGG)\n",
    "def value_max(value):\n",
    "    return max(value)\n",
    "\n",
    "@pandas_udf(\"array<string>\", PandasUDFType.GROUPED_AGG)\n",
    "def info_min(info):\n",
    "    temp = [value for value in info if value.startswith(\"END=\") == False]\n",
    "    # info 값 수정\n",
    "    return temp\n",
    "\n",
    "@pandas_udf(\"String\", PandasUDFType.GROUPED_AGG)\n",
    "def format_value(value): \n",
    "    # ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=\"Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias.\">\n",
    "    if len(value) == 1:\n",
    "        value.append(\"GT:DP:GQ:MIN_DP:PL\")\n",
    "    def format_reduce(left, right):\n",
    "        left, right = left.split(\":\"), right.split(\":\")\n",
    "        if len(left) <= len(right):        \n",
    "            temp = copy.deepcopy(right)\n",
    "            right = copy.deepcopy(left)\n",
    "            left = copy.deepcopy(temp)\n",
    "        for value in right:\n",
    "            if value not in left:\n",
    "                left.append(value)\n",
    "        return \":\".join(left)\n",
    "    return str(reduce(format_reduce, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).orderBy(F.col(\"mean_udf(v)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs = \"hdfs://master:9000\"\n",
    "hdfs_list = hadoop_list(3, \"/raw_data/gvcf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf = union_all([preVCF(hdfs + sample.decode(\"UTF-8\"), 0, spark) for sample in hdfs_list]).cache()\n",
    "vcf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = vcf.groupby(\"#CHROM\", \"POS\").agg(value_collect(vcf['ID']), value_collect(vcf[\"REF\"]), value_collect(vcf[\"ALT\"]), value_collect(vcf[\"INFO\"]), \n",
    "                                value_collect(vcf[\"FORMAT\"])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
