{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Spark using Standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, desc, asc, coalesce, broadcast\n",
    "from pyspark import Row\n",
    "import re\n",
    "\n",
    "# Start for Spark Session\n",
    "spark = SparkSession.builder.master(\"spark://master:7077\")\\\n",
    "                        .appName(\"gVCF_combine\")\\\n",
    "                        .config(\"spark.driver.memory\", \"8G\")\\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"8G\")\\\n",
    "                        .config(\"spark.executor.memory\", \"24G\")\\\n",
    "                        .config(\"spark.executor.core\", 3)\\\n",
    "                        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "                        .config(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")\\\n",
    "                        .config(\"spark.network.timeout\", 10000000)\\\n",
    "                        .config(\"spark.sql.shuffle.partitions\", 40)\\\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCF merge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chr_remove(chrom):\n",
    "    chrom = re.sub(\"chr\", \"\", chrom) # \"chr\" to \"\"\n",
    "    if chrom == \"X\": \n",
    "        chrom = \"23\"\n",
    "    elif chrom == \"Y\": \n",
    "        chrom = \"24\"\n",
    "    elif chrom == \"XY\" or chrom == \"M\": \n",
    "        chrom = \"-99\"\n",
    "    return chrom\n",
    "chr_remove_udf = udf(chr_remove)\n",
    "\n",
    "def alt_filter(row):\n",
    "    if \",\" in row[4]:\n",
    "        temp = row[4].split(\",\")\n",
    "        temp.sort()\n",
    "        row_new = [\",\".join(temp), ]\n",
    "        \n",
    "        return row[:4] + row_new + row[5:]\n",
    "    \n",
    "    else:\n",
    "        return row\n",
    "\n",
    "def preVCF(hdfs, flag, spark): # hdfs://, flag 0 == lhs, 1 == rhs\n",
    "    vcf = spark.sparkContext.textFile(hdfs).map(lambda x : x.split(\"\\t\"))\n",
    "    header = vcf.first()\n",
    "    step1 = vcf.filter(lambda row : row != header).map(alt_filter).toDF(header)\n",
    "    return_vcf = step1.select(chr_remove_udf(step1[\"#CHROM\"]).cast(\"Integer\").alias(\"CHROM\"), \"*\")\\\n",
    "                      .drop(col(\"#CHROM\")).filter(col(\"FILTER\") == \"PASS\")\n",
    "    if flag == 1:\n",
    "        for index in range(len(return_vcf.columns[:9])):\n",
    "            return_vcf = return_vcf.withColumnRenamed(return_vcf.columns[index], return_vcf.columns[index] + \"_temp\") \n",
    "    return return_vcf\n",
    "\n",
    "def rowTodict(format_, row):\n",
    "    return_col = []\n",
    "    for ref in row:\n",
    "        temp_dict = dict()\n",
    "        temp = ref.split(\":\")\n",
    "        for index in range(len(temp)):\n",
    "            temp_dict[format_[index]] = temp[index]\n",
    "        return_col.append(temp_dict)\n",
    "    return return_col\n",
    "\n",
    "def dictToFormat(col_value, d_format):\n",
    "    result_return = []\n",
    "    for temp in col_value:\n",
    "        temp_col = []\n",
    "        for keys in d_format:\n",
    "            if keys in temp:\n",
    "                temp_col.append(temp[keys])\n",
    "            else:\n",
    "                temp_col.append(\".\")\n",
    "        result_return.append(\":\".join(temp_col))\n",
    "    return tuple(result_return)\n",
    "\n",
    "def selectCol(row, lhs_len, rhs_len):\n",
    "    # INFO re      \n",
    "    AC, AN = 0, 0 \n",
    "    \n",
    "    if row[9] == None :\n",
    "        GT = row[lhs_len + 9:]\n",
    "    elif row[lhs_len + 9] == None :\n",
    "        GT = row[9:lhs_len]\n",
    "    else:\n",
    "        GT = row[9:lhs_len]+row[lhs_len + 9:]\n",
    "        \n",
    "    for temp in GT:\n",
    "        if temp == None:\n",
    "            break\n",
    "        else:\n",
    "            if \"0/1:\" in temp:\n",
    "                AC += 1\n",
    "                AN += 1\n",
    "            elif \"1/1:\" in temp:\n",
    "                AC += 2\n",
    "                AN += 1\n",
    "            elif \"0/0:\" in temp:\n",
    "                AN += 1\n",
    "    \n",
    "    # rhs가 null\n",
    "    if(row.CHROM_temp == None):\n",
    "        temp = tuple()\n",
    "        for ref in range(rhs_len - 9):\n",
    "            temp += (\"0/0\",) # GC\n",
    "            AN += 1\n",
    "\n",
    "        # info\n",
    "        AN *= 2\n",
    "        info = (\"AC=\"+str(AC)+\";AN=\"+str(AN)+\";SF=0\",)\n",
    "        return row[:5] + (float(row.QUAL),) + (row.FILTER, ) + info + (row[8],) + row[9:lhs_len] + temp\n",
    "    \n",
    "    # lhs가 null\n",
    "    elif(row.CHROM == None):\n",
    "        temp = tuple()\n",
    "        for ref in range(lhs_len - 9):\n",
    "            temp += (\"0/0\",) # GC\n",
    "            AN += 1\n",
    "        \n",
    "        # info\n",
    "        AN *= 2\n",
    "        info = (\"AC=\"+str(AC)+\";AN=\"+str(AN)+\";SF=1\",)\n",
    "        return row[lhs_len:lhs_len + 5] + (float(row.QUAL_temp), ) + (row.FILTER_temp, ) + info + (row.FORMAT_temp,) + temp + row[lhs_len + 9:]\n",
    "    \n",
    "    # case, control 둘다 존재\n",
    "    else:\n",
    "        # QUAL re-calculation\n",
    "        format_, lhs_format, rhs_format = row[8].split(\":\")+row[lhs_len + 8].split(\":\"), row[8].split(\":\"), row[lhs_len + 8].split(\":\")\n",
    "        dup_format, lhs_col, rhs_col = [], rowTodict(lhs_format, row[9:lhs_len]), rowTodict(rhs_format, row[lhs_len + 9:])\n",
    "        \n",
    "        # format duplicate\n",
    "        for dup in format_:\n",
    "            if dup not in dup_format:\n",
    "                dup_format.append(dup)\n",
    "        \n",
    "        result_lhs, result_rhs = dictToFormat(lhs_col, dup_format), dictToFormat(rhs_col, dup_format)\n",
    "        \n",
    "        # qual re-calcualtion # 100\n",
    "        col_total = lhs_len + rhs_len - 18\n",
    "        lhs_QUAL = float(row.QUAL) * ((lhs_len - 9) / col_total)\n",
    "        rhs_QUAL = float(row.QUAL_temp) * ((rhs_len - 9) / col_total)\n",
    "        QUAL = lhs_QUAL + rhs_QUAL\n",
    "        \n",
    "        # info\n",
    "        AN *= 2\n",
    "        info = (\"AC=\"+str(AC)+\";AN=\"+str(AN)+\";SF=0,1\",)        \n",
    "        \n",
    "        #return row[:5]+(QUAL,)+(row[6],)+info+(row[8],)+row[9:lhs_len]+row[lhs_len + 9:]\n",
    "        return row[:5]+(QUAL,)+(row[6],)+info+(\":\".join(dup_format), )+result_lhs + result_rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run VCF merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load case.vcf from HDFS\n",
    "case = preVCF(\"hdfs://master:9000/raw_data/vcf/case_merge.vcf\", 0, spark)\n",
    "control = preVCF(\"hdfs://master:9000/raw_data/vcf/control.vcf\", 1, spark)\n",
    "\n",
    "# case & control indexing\n",
    "case_col = len(case.columns)\n",
    "control_col = len(control.columns)\n",
    "\n",
    "# merge schema\n",
    "col = case.columns + control.columns\n",
    "header = col[:case_col] + col[case_col + 9:]\n",
    "\n",
    "### join expresion\n",
    "joinEX = [\n",
    "              case['CHROM'] == control['CHROM_temp'],\n",
    "              case['POS'] == control['POS_temp'],\n",
    "              case['REF'] == control['REF_temp']\n",
    "         ]\n",
    "join_result = case.join(control, joinEX, 'full')\n",
    "\n",
    "# write delim \\t\n",
    "join_result.rdd.map(lambda row : selectCol(row, case_col, control_col))\\\n",
    "               .toDF(header).dropDuplicates(['CHROM', 'POS'])\\\n",
    "               .write.mode('overwrite').option(\"delimiter\", \"\\t\").csv(\"hdfs://master:9000/raw_data/vcf/out/vcf_merge.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
