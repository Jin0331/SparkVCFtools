{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark & python function\n",
    "import pandas\n",
    "import pyarrow\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://master:7077\")\\\n",
    "                        .appName(\"gVCF_combine\")\\\n",
    "                        .config(\"spark.executor.memory\", \"24G\")\\\n",
    "                        .config(\"spark.executor.core\", \"3\")\\\n",
    "                        .config(\"spark.sql.shuffle.partitions\",20)\\\n",
    "                        .config(\"spark.driver.memory\", \"8G\")\\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"8G\")\\\n",
    "                        .config(\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "                        .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1')\\\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preVCF(hdfs, flag, spark):\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    # drop ---> QUAL FILTER column\n",
    "    header_contig = vcf.filter(lambda x : re.match(\"^#\", x))\n",
    "    col_name = vcf.filter(lambda x : x.startswith(\"#CHROM\")).first().split(\"\\t\")\n",
    "    vcf_data = vcf.filter(lambda x : re.match(\"[^#][^#]\", x))\\\n",
    "                       .map(lambda x : x.split(\"\\t\"))\\\n",
    "                       .toDF(col_name)\\\n",
    "                       .withColumn(\"POS\", F.col(\"POS\").cast(IntegerType()))\n",
    "    \n",
    "    if flag == 1:\n",
    "        for index in range(len(vcf_data.columns[:9])):\n",
    "            compared_arr = [\"#CHROM\", \"POS\", \"REF\"]\n",
    "            if vcf_data.columns[index] in compared_arr:\n",
    "                continue\n",
    "            vcf_data = vcf_data.withColumnRenamed(vcf_data.columns[index], vcf_data.columns[index] + \"_temp\") \n",
    "    \n",
    "    return vcf_data\n",
    "\n",
    "def hadoop_list(length, hdfs):\n",
    "    args = \"hdfs dfs -ls \"+ hdfs +\" | awk '{print $8}'\"\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    all_dart_dirs = s_output.split()\n",
    "    \n",
    "    return all_dart_dirs[:length]\n",
    "\n",
    "def selectNotNull(left, right):\n",
    "    if left == None:\n",
    "        return right\n",
    "    else:\n",
    "        return left\n",
    "selectNotNull_u = udf(selectNotNull, returnType=StringType())   \n",
    "\n",
    "def qual_filter(none = None):\n",
    "    return \".\"\n",
    "qual_filter_u = udf(qual_filter, returnType=StringType())\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for index in range(0, len(lst), n):\n",
    "        yield lst[index:index + n]\n",
    "\n",
    "\"\"\"\n",
    "def chunks(lst, n):\n",
    "    for index in range(0, len(lst), n):\n",
    "        if index == 0:\n",
    "            yield lst[index:index + 3]\n",
    "        else :\n",
    "            yield lst[index:index + n]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "hdfs = \"hdfs://master:9000\"\n",
    "hdfs_list = hadoop_list(6, \"/raw_data/gvcf\")\n",
    "\n",
    "w = Window.partitionBy(\"#CHROM\").orderBy(\"POS\")\n",
    "sample_w = Window.partitionBy(\"#CHROM\").orderBy(\"POS\").rangeBetween(Window.unboundedPreceding, Window.currentRow)   \n",
    "\n",
    "# all files join\n",
    "for index in range(len(hdfs_list)):\n",
    "    if index == 0:\n",
    "        join_vcf = preVCF(hdfs + hdfs_list[index].decode(\"UTF-8\"), 0, spark)\n",
    "    else:\n",
    "        join_vcf = join_vcf.join(preVCF(hdfs + hdfs_list[index].decode(\"UTF-8\"), 1, spark), [\"#CHROM\", \"POS\", \"REF\"], \"full\")\\\n",
    "            .withColumn(\"ID\", when(F.col(\"ID\").isNull(), F.col(\"ID_temp\")).otherwise(F.col(\"ID\")))\\\n",
    "            .withColumn(\"ALT\",when(F.col(\"ALT\").isNull(), F.col(\"ALT_temp\")).otherwise(F.col(\"ALT\")))\\\n",
    "            .withColumn(\"FORMAT\", when(F.col(\"FORMAT\").isNull(), F.col(\"FORMAT_temp\")).otherwise(F.col(\"FORMAT\")))\\\n",
    "            .withColumn(\"QUAL\", F.lit(\".\")).withColumn(\"FILTER\", F.lit(\".\"))\\\n",
    "            .withColumn(\"INFO\", when(F.col(\"INFO\").startswith(\"END\") == False, F.col(\"INFO\"))\\\n",
    "                        .when(F.col(\"INFO_temp\").startswith(\"END\") == False, F.col(\"INFO_temp\")))\\\n",
    "            .drop(\"INFO_temp\", \"ID_temp\", \"ALT_temp\", \"FORMAT_temp\", \"QUAL_temp\", \"FILTER_temp\")\n",
    "\n",
    "join_vcf = join_vcf.withColumn(\"INFO\", when(F.col(\"INFO\").isNull(), F.concat(F.lit(\"END=\"), F.lead(\"POS\", 1).over(w) - 1))\\\n",
    "                              .otherwise(F.col(\"INFO\"))).cache()\n",
    "join_vcf.count()\n",
    "\n",
    "# per sample value update(block) using SQL window\n",
    "sample_list = []\n",
    "count = 0\n",
    "for sample_name in chunks(join_vcf.columns[9:], 3):\n",
    "    if count == 0:\n",
    "        sample_list.append(join_vcf.select(join_vcf.columns[:9] + [col for col in sample_name]))\n",
    "    else :\n",
    "        sample_list.append(join_vcf.select([\"#CHROM\",\"POS\"] + [col for col in sample_name]))\n",
    "        \n",
    "    for index in range(len(sample_name)): \n",
    "        sample_list[count] = sample_list[count].withColumn(sample_name[index], F.last(sample_name[index], ignorenulls=True).over(sample_w))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list[1].write.format(\"jdbc\")\\\n",
    "           .option(\"url\", \"jdbc:postgresql://210.115.229.97:5432/gvcf\")\\\n",
    "           .option(\"dbtable\", \"sample_gvcf.sample\")\\\n",
    "           .option(\"user\", \"postgres\")\\\n",
    "           .option(\"password\", \"sempre813!\")\\\n",
    "           .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
