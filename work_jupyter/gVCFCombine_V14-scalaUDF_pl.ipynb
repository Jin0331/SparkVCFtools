{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Spark function\n",
    "from pyspark import Row, StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, explode, array, when\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, BooleanType, MapType\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Python function\n",
    "import re\n",
    "import subprocess\n",
    "from functools import reduce \n",
    "import copy\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "appname = input(\"appname, folder name : \")\n",
    "folder_name = copy.deepcopy(appname) \n",
    "gvcf_count = int(input(\"gvcf count : \"))\n",
    "\n",
    "# Start for Spark Session\n",
    "spark = SparkSession.builder.master(\"spark://master:7077\")\\\n",
    "                        .appName(appname)\\\n",
    "                        .config(\"spark.driver.memory\", \"8G\")\\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"5G\")\\\n",
    "                        .config(\"spark.executor.memory\", \"25G\")\\\n",
    "                        .config(\"spark.memory.fraction\", 0.1)\\\n",
    "                        .config(\"spark.sql.shuffle.partitions\", 100)\\\n",
    "                        .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "                        .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "                        .config(\"spark.cleaner.periodicGC.interval\", \"15min\")\\\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hadoop_list(length, hdfs):\n",
    "    args = \"hdfs dfs -ls \"+ hdfs +\" | awk '{print $8}'\"\n",
    "    proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    s_output, s_err = proc.communicate()\n",
    "    all_dart_dirs = s_output.split()\n",
    "    return all_dart_dirs[:length]\n",
    "\n",
    "def headerVCF(hdfs, spark):\n",
    "    col_name = [\"key\", \"value\"]\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    vcf = vcf.filter(lambda x : re.match(\"^##\", x))\\\n",
    "             .map(lambda x : x.split(\"=\", 1))\\\n",
    "             .toDF(col_name)\n",
    "    return vcf\n",
    "\n",
    "def select_list(value, index):\n",
    "    return value[index]\n",
    "select_list = udf(select_list, StringType())\n",
    "\n",
    "\n",
    "def preVCF(hdfs, spark):\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    col_name = vcf.filter(lambda x : x.startswith(\"#CHROM\")).first().split(\"\\t\")\n",
    "    vcf_data = vcf.filter(lambda x : re.match(\"[^#][^#]\", x))\\\n",
    "                       .map(lambda x : x.split(\"\\t\"))\\\n",
    "                       .toDF(col_name)\\\n",
    "                       .withColumn(\"POS\", F.col(\"POS\").cast(IntegerType()))\\\n",
    "                       .withColumn(\"ALT\", F.array_remove(F.split(F.col(\"ALT\"), \",\"), \"<NON_REF>\"))\\\n",
    "                       .withColumn(\"FORMAT\", F.array_remove(F.split(F.col(\"FORMAT\"), \":\"), \"GT\"))\\\n",
    "                       .withColumn(\"INFO\", when(F.col(\"INFO\").startswith(\"END=\"), None).otherwise(F.col(\"INFO\")))\n",
    "    \n",
    "    sample_name = vcf_data.columns[-1]\n",
    "    vcf_data = vcf_data.drop(\"QUAL\", \"FILTER\", sample_name)\n",
    "    \n",
    "    for index in range(len(vcf_data.columns)):\n",
    "        compared_arr = [\"#CHROM\", \"POS\", \"REF\"]\n",
    "        if vcf_data.columns[index] not in compared_arr:\n",
    "            if vcf_data.columns[index] != \"ALT\":\n",
    "                vcf_data = vcf_data.withColumn(vcf_data.columns[index], F.array(vcf_data.columns[index]))\n",
    "            vcf_data = vcf_data.withColumnRenamed(vcf_data.columns[index], vcf_data.columns[index] + \"_\" + sample_name)     \n",
    "            \n",
    "    vcf_data = vcf_data.withColumnRenamed(\"REF\", \"REF_\" + sample_name)    \n",
    "    vcf_data = vcf_data.withColumn(\"count\", F.length(vcf_data.columns[3]))\n",
    "    \n",
    "    # window & split col parameter \n",
    "    sample_ref = vcf_data.columns[3]\n",
    "    indel_window = Window.partitionBy(\"#CHROM\").orderBy(\"POS\")\n",
    "    split_col = F.split(\"REF_temp\", '_')\n",
    "    ###\n",
    "    \n",
    "    vcf_data = vcf_data.withColumn(\"count\", when(F.col(\"count\") >= 2, F.lead(\"POS\", 1).over(indel_window) - F.col(\"POS\") - F.lit(1))\\\n",
    "                                           .otherwise(F.lit(0)))\n",
    "\n",
    "    not_indel = vcf_data.drop(\"count\").withColumn(sample_ref, F.array(F.col(sample_ref)))\n",
    "    indel = vcf_data.filter(F.col(\"count\") >= 1)\\\n",
    "                .withColumn(sample_ref, ref_melt(F.col(sample_ref), F.col(\"count\"))).drop(\"count\")\\\n",
    "                .withColumn(sample_ref, explode(F.col(sample_ref))).withColumnRenamed(sample_ref, \"REF_temp\")\\\n",
    "                .withColumn(sample_ref, F.array(split_col.getItem(0))).withColumn('POS_var', split_col.getItem(1))\\\n",
    "                .drop(F.col(\"REF_temp\")).withColumn(\"POS\", (F.col(\"POS\") + F.col(\"POS_var\")).cast(IntegerType()))\\\n",
    "                .drop(F.col(\"POS_var\"))\\\n",
    "                .withColumn(vcf_data.columns[2], F.array(F.lit(\".\")))\\\n",
    "                .withColumn(vcf_data.columns[4], F.array(F.lit(\"*\")))\n",
    "    \n",
    "    return not_indel.unionByName(indel) \n",
    "\n",
    "def ref_melt(ref, count):\n",
    "    temp = list(ref)[1:count + 1]\n",
    "    return_str = []\n",
    "    for num in range(len(temp)):\n",
    "        return_str.append(temp[num] + \"_\" + str(int(num + 1)))\n",
    "    return return_str\n",
    "ref_melt = udf(ref_melt, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def sampleVCF(hdfs, spark):\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    col_name = vcf.filter(lambda x : x.startswith(\"#CHROM\")).first().split(\"\\t\")\n",
    "    vcf_data = vcf.filter(lambda x : re.match(\"[^#][^#]\", x))\\\n",
    "                       .map(lambda x : x.split(\"\\t\"))\\\n",
    "                       .toDF(col_name)\\\n",
    "                       .withColumn(\"POS\", F.col(\"POS\").cast(IntegerType()))\n",
    "    \n",
    "    sample_name = vcf_data.columns[-1]\n",
    "    vcf_data = vcf_data.withColumnRenamed(\"FORMAT\", \"FORMAT_\" + sample_name)\\\n",
    "                       .withColumn(sample_name, F.array_join(F.slice(F.split(F.col(sample_name), \":\"), 2, 8), \":\"))\n",
    "    return vcf_data.select(F.col(\"#CHROM\").alias(\"CHROM\"), F.col(\"POS\"), \n",
    "                           F.array(F.col(\"FORMAT_\"+sample_name), F.col(\"ALT\"), F.col(sample_name)).alias(sample_name))\n",
    "\n",
    "def gatkVCF(hdfs, spark):\n",
    "    vcf = spark.sparkContext.textFile(hdfs)\n",
    "    #header_contig = vcf.filter(lambda x : re.match(\"^#\", x))\n",
    "    col_name = vcf.filter(lambda x : x.startswith(\"#CHROM\")).first().split(\"\\t\")\n",
    "    vcf_data = vcf.filter(lambda x : re.match(\"[^#][^#]\", x))\\\n",
    "                       .map(lambda x : x.split(\"\\t\"))\\\n",
    "                       .toDF(col_name)\n",
    "    return vcf_data\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for index in range(0, len(lst), n):\n",
    "        yield lst[index:index + n]\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionByName, dfs) \n",
    "\n",
    "def reduce_join(left, right):   \n",
    "    return_vcf = left.join(right, [\"#CHROM\", \"POS\"], \"full\")\n",
    "\n",
    "    ###\n",
    "    remove_colname = right.columns[2:]\n",
    "    l_name = left.columns\n",
    "    r_name = right.columns\n",
    "    v_name = return_vcf.columns\n",
    "    name_list = [\"REF\", \"ID\", \"ALT\", \"INFO\", \"FORMAT\"]\n",
    "    \n",
    "    for name in name_list:\n",
    "        if name == \"INFO\":\n",
    "            return_vcf = return_vcf.withColumn(column_name(l_name, name)[0], \n",
    "                                       when(F.isnull(column_name(l_name, name)[0]), F.col(column_name(r_name, name)[0]))\\\n",
    "                                       .when(F.isnull(column_name(r_name, name)[0]), F.col(column_name(l_name, name)[0]))\n",
    "                                       .otherwise(F.array_union(*column_name(v_name, name))))\n",
    "        else :\n",
    "            return_vcf = return_vcf.withColumn(column_name(l_name, name)[0], \n",
    "                                        when(F.isnull(column_name(l_name, name)[0]), F.col(column_name(r_name, name)[0]))\\\n",
    "                                        .when(F.isnull(column_name(r_name, name)[0]), F.col(column_name(l_name, name)[0]))\n",
    "                                        .otherwise(F.array_union(*column_name(v_name, name))))\n",
    "    return_vcf = return_vcf.drop(*remove_colname)\n",
    "                                        \n",
    "    return return_vcf\n",
    "\n",
    "def column_rename(vcf):\n",
    "    name_list = [\"REF\", \"ID\", \"ALT\", \"INFO\", \"FORMAT\"]\n",
    "    for name in name_list:\n",
    "        vcf = vcf.withColumnRenamed(column_name(vcf.columns, name)[0], name)\n",
    "    return vcf\n",
    "    \n",
    "def reduce_inner_join(left, right):   \n",
    "    return_vcf = left.join(right, [\"#CHROM\", \"POS\"], \"inner\")\n",
    "    return return_vcf\n",
    "\n",
    "def column_name(df_col, name):\n",
    "    return_list = []\n",
    "    for col in df_col:\n",
    "        if col.startswith(name):\n",
    "            return_list.append(col)\n",
    "    return return_list\n",
    "\n",
    "def column_revalue(vcf):\n",
    "    # info 값 수정 필요\n",
    "    name_list = [\"ID\", \"REF\",\"ALT\", \"INFO\", \"FORMAT\"]\n",
    "    for name in name_list:\n",
    "        if name == \"FORMAT\":\n",
    "            vcf = vcf.withColumn(name, F.array_sort(F.array_distinct(F.flatten(F.col(name)))))\n",
    "            vcf = vcf.withColumn(name, F.concat(F.lit(\"GT:\"), F.array_join(F.col(name), \":\")))\n",
    "        elif name == \"ALT\":\n",
    "            vcf = vcf.withColumn(name, F.array_join(F.concat(F.col(name), F.array(F.lit(\"<NON_REF>\"))), \",\"))\n",
    "        else:\n",
    "            vcf = vcf.withColumn(name, F.array_max(F.col(name)))\n",
    "            \n",
    "    return vcf\n",
    "\n",
    "# 중복찾기\n",
    "def find_duplicate(temp):\n",
    "    return temp.groupBy(F.col(\"#CHROM\"), F.col(\"POS\")).agg((F.count(\"*\")>1).cast(\"int\").alias(\"e\"))\\\n",
    "               .orderBy(F.col(\"e\"), ascending=False)\n",
    "# 10개 기준임.\n",
    "def join_split(v_list):\n",
    "    stage1_list = list(chunks(v_list, 5))\n",
    "    stage1 = []\n",
    "    for vcf in stage1_list:\n",
    "        stage1.append(reduce(reduce_join, vcf))\n",
    "    return reduce(reduce_join, stage1)\n",
    "\n",
    "def join_inner(v_list):\n",
    "    if len(v_list) <= 1:\n",
    "        return v_list[0]\n",
    "    else :\n",
    "        return v_list[0].join(v_list[1], [\"#CHROM\", \"POS\"], \"inner\")\n",
    "\n",
    "def join_split_inner(v_list, num):\n",
    "    stage1_list = list(chunks(v_list, num))\n",
    "    stage1 = []\n",
    "    for vcf in stage1_list:\n",
    "        temp = list(chunks(vcf, 2))\n",
    "        temp = list(map(join_inner, temp))\n",
    "        stage1.append(reduce(reduce_inner_join, temp))\n",
    "    return stage1\n",
    "\n",
    "def join_split_inner_2(v_list, num):\n",
    "    stage1_list = list(chunks(v_list, num))\n",
    "    stage1 = []\n",
    "    for vcf in stage1_list:\n",
    "        stage1.append(reduce(reduce_inner_join, vcf))\n",
    "    return stage1\n",
    "\n",
    "def vcf_join(v_list):\n",
    "    chunks_list = list(chunks(v_list, 10))\n",
    "    map_list = list(map(join_split, chunks_list))\n",
    "    if len(map_list) <= 1:\n",
    "        return map_list[0]\n",
    "    elif len(map_list) == 2:\n",
    "        return reduce(reduce_join, map_list)\n",
    "    else:\n",
    "        flag = True\n",
    "        while flag == True:\n",
    "            if len(map_list) == 2:\n",
    "                flag = False\n",
    "            else :\n",
    "                map_list = list(chunks(map_list, 2))\n",
    "                map_list = list(map(join_split, map_list))\n",
    "        return reduce(reduce_join, map_list)    \n",
    "\n",
    "def headerWrite(vcf, vcf_list, index, spark):\n",
    "    contig = vcf_list[index].toPandas()\n",
    "    contig_dup = contig[\"key\"].drop_duplicates().tolist()\n",
    "    contig_len = list(range(len(contig_dup)))\n",
    "    contig_DF = spark.createDataFrame(list(zip(contig_dup, contig_len)), [\"key\", \"index\"])\n",
    "\n",
    "    return vcf.dropDuplicates([\"value\"]).join(contig_DF, [\"key\"], \"inner\")\\\n",
    "              .select(F.concat_ws(\"=\", F.col(\"key\"), F.col(\"value\")).alias(\"meta\"))\\\n",
    "              .coalesce(1)\n",
    "\n",
    "def parquet_revalue(vcf, indel_com):\n",
    "    temp = indel_com.join(vcf, [\"CHROM\", \"POS\"], \"full\")\n",
    "    sample_name = temp.columns[-1]\n",
    "    \n",
    "    sample_w = Window.partitionBy(F.col(\"CHROM\")).orderBy(F.col(\"POS\")).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    temp = temp.withColumn(sample_name, F.last(sample_name, ignorenulls=True).over(sample_w))\n",
    "    \n",
    "    \n",
    "    # scala UDF\n",
    "    null_not_value = temp.filter(F.col(\"FORMAT\") != F.element_at(F.col(sample_name), 1))\\\n",
    "                         .selectExpr(\"CHROM\", \"POS\",\"revaluationPL(FORMAT, value, {}) as {}\".format(sample_name, sample_name))\n",
    "        \n",
    "    null_value = temp.filter(F.col(\"FORMAT\") == F.element_at(F.col(sample_name), 1)).drop(\"FORMAT\", \"value\")\\\n",
    "                     .withColumn(sample_name, F.concat(F.lit(\"./.:\"), F.element_at(F.col(sample_name), 3)))\n",
    "    \n",
    "    value_union = null_not_value.union(null_value).withColumnRenamed(\"CHROM\", \"#CHROM\")\n",
    "    return value_union.repartition(F.col(\"#CHROM\"))\n",
    "\n",
    "def parquet_revalue_not_union(vcf, indel_com):\n",
    "    temp = indel_com.join(vcf, [\"CHROM\", \"POS\"], \"left\")\n",
    "    sample_name = temp.columns[-1]\n",
    "    \n",
    "    sample_w = Window.partitionBy(F.col(\"CHROM\")).orderBy(F.col(\"POS\")).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    temp = temp.withColumn(sample_name, F.last(sample_name, ignorenulls=True).over(sample_w))\n",
    "    \n",
    "    # scala UDF\n",
    "    null_not_value = temp.withColumn(sample_name, when(F.col(\"FORMAT\") != F.element_at(F.col(sample_name), 1), \n",
    "                                                       F.expr(\"revaluationPL(FORMAT, value, {})\".format(sample_name)))\\\n",
    "                                                  .otherwise(F.concat(F.lit(\"./.:\"), F.element_at(F.col(sample_name), 3))))\\\n",
    "                         .drop(\"FORMAT\", \"value\").withColumnRenamed(\"CHROM\", \"#CHROM\")\n",
    "    return null_not_value\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet write for info filed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcf load\n",
    "hdfs = \"hdfs://master:9000\"\n",
    "hdfs_list = hadoop_list(gvcf_count, \"/raw_data/gvcf\")\n",
    "vcf_header = list()\n",
    "vcf_list = list()\n",
    "\n",
    "for index in range(len(hdfs_list)):\n",
    "    vcf_header.append(headerVCF(hdfs + hdfs_list[index].decode(\"UTF-8\"), spark))\n",
    "    vcf_list.append(preVCF(hdfs + hdfs_list[index].decode(\"UTF-8\"), spark).persist(StorageLevel.MEMORY_ONLY))\n",
    "header = unionAll(*vcf_header).persist(StorageLevel.MEMORY_ONLY)\n",
    "header.count()\n",
    "    \n",
    "headerWrite(header, vcf_header, 0, spark).write.format(\"text\").option(\"header\", \"false\").mode(\"append\")\\\n",
    "                                  .save(\"/raw_data/output/gvcf_output/\"+ folder_name + \"//\" + \"gvcf_meta.txt\")\n",
    "\n",
    "info_last = header.filter(F.col(\"key\") == \"##contig\").select(\"value\").dropDuplicates([\"value\"])\\\n",
    "      .withColumn(\"value\", F.split(F.col(\"value\"), \",\"))\\\n",
    "      .select(F.regexp_replace(select_list(F.col(\"value\"), F.lit(0)), \"<ID=\", \"\").alias(\"#CHROM\"), \n",
    "              F.concat(F.lit(\"END=\"), F.regexp_replace(select_list(F.col(\"value\"), F.lit(1)), \"length=\", \"\")).alias(\"INFO\"))\n",
    "\n",
    "vcf = column_rename(vcf_join(vcf_list))\n",
    "vcf = column_revalue(vcf).persist(StorageLevel.MEMORY_ONLY)\n",
    "vcf.count()\n",
    "\n",
    "#window\n",
    "# isNull() ---> isnull()\n",
    "info_window = Window.partitionBy(\"#CHROM\").orderBy(\"POS\")\n",
    "vcf_not_indel = vcf.withColumn(\"INFO\", when(F.col(\"INFO\").isNull(), F.concat(F.lit(\"END=\"), F.lead(\"POS\", 1).over(info_window) - 1))\\\n",
    "                              .otherwise(F.col(\"INFO\")))\n",
    "\n",
    "not_last = vcf_not_indel.filter(F.col(\"INFO\").isNotNull())\n",
    "last = vcf_not_indel.filter(F.col(\"INFO\").isNull())\\\n",
    "                 .drop(\"INFO\").join(info_last, [\"#CHROM\"], \"inner\")\\\n",
    "                 .select(\"#CHROM\", \"POS\", \"ID\", \"REF\", \"ALT\", \"INFO\", \"FORMAT\")\n",
    "\n",
    "unionAll(*[not_last, last])\\\n",
    "                 .orderBy(F.col(\"#CHROM\"), F.col(\"POS\"))\\\n",
    "                 .dropDuplicates([\"#CHROM\", \"POS\"])\\\n",
    "                 .write.mode('overwrite')\\\n",
    "                 .parquet(\"/raw_data/output/gvcf_output/\" + folder_name + \"//info.g.vcf\")\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet write for sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "num = 10\n",
    "part_num = 80\n",
    "# Start for Spark Session with write\n",
    "spark = SparkSession.builder.master(\"spark://master:7077\")\\\n",
    "                        .appName(appname + \"_sample\" + str(num) + \"_\" + str(part_num))\\\n",
    "                        .config(\"spark.driver.memory\", \"8G\")\\\n",
    "                        .config(\"spark.driver.maxResultSize\", \"5G\")\\\n",
    "                        .config(\"spark.executor.memory\", \"25G\")\\\n",
    "                        .config(\"spark.sql.shuffle.partitions\", part_num)\\\n",
    "                        .config(\"spark.eventLog.enabled\", \"true\")\\\n",
    "                        .config(\"spark.memory.fraction\", 0.1)\\\n",
    "                        .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "                        .getOrCreate()\n",
    "# scala UDF\n",
    "spark.udf.registerJavaFunction(\"revaluationPL\", \"scalaUDF.revaluationPL3\", StringType())\n",
    "spark.udf.registerJavaFunction(\"refalt2array\", \"scalaUDF.refalt2array\", ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample parquet write\n",
    "hdfs = \"hdfs://master:9000\"\n",
    "hdfs_list = hadoop_list(gvcf_count, \"/raw_data/gvcf\")\n",
    "vcf_list = list()\n",
    "for index in range(len(hdfs_list)):\n",
    "    vcf_list.append(sampleVCF(hdfs + hdfs_list[index].decode(\"UTF-8\"), spark))\n",
    "    \n",
    "indel_com = spark.read.parquet(\"/raw_data/output/gvcf_output/\" + folder_name + \"//info.g.vcf\")\\\n",
    "                 .select(F.col(\"#CHROM\").alias(\"CHROM\"), \"POS\", \"FORMAT\", F.concat(F.array(\"REF\"),F.split(F.col(\"ALT\"), \",\")).alias(\"value\"))\\\n",
    "                 .orderBy(F.col(\"CHROM\"), F.col(\"POS\"))\\\n",
    "                 .persist(StorageLevel.MEMORY_ONLY)\n",
    "indel_com.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parquet_list = list(map(lambda arg : parquet_revalue_not_union(arg, indel_com), vcf_list))\n",
    "for parquet in join_split_inner(parquet_list, num):\n",
    "    parquet.write.mode('overwrite')\\\n",
    "            .parquet(\"/raw_data/output/gvcf_output/\"+ folder_name + \"//\" + \"sample_\" + str(cnt) + \".g.vcf\")\n",
    "    cnt += num\n",
    "    \n",
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
